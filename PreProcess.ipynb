{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import h5py\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import math\n",
    "import re\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "preprocessed_path = '/datasets/home/17/717/yuz310/pre/resnet-14x14.h5'\n",
    "vocabulary_path='/datasets/home/17/717/yuz310/pre/voca.json'\n",
    "train_questions = '/datasets/ee285f-public/VQA2017/v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "train_answers = '/datasets/ee285f-public/VQA2017/v2_mscoco_train2014_annotations.json'\n",
    "train_image_path = \"/datasets/ee285f-public/VQA2017/train2014/\"\n",
    "val_image_path = \"/datasets/ee285f-public/VQA2017/val2014/\"\n",
    "\n",
    "image_size = 448  # scale shorter end of image to this size and centre crop\n",
    "output_size = image_size // 32  # size of the feature maps after processing through a network\n",
    "output_features = 2048  # number of feature maps thereof\n",
    "central_fraction = 0.875  # only take this much of the centre when scaling and centre cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pretrained_ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pretrained_ResNet, self).__init__()\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        def save_output(module, input, output):\n",
    "            self.buffer = output\n",
    "        self.resnet.layer4.register_forward_hook(save_output)\n",
    "        self.resnet.fc = nn.Linear(131072, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.resnet(x)\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoImages(data.Dataset):\n",
    "    \"\"\" Dataset for MSCOCO images located in a folder on the filesystem \"\"\"\n",
    "    def __init__(self, path):\n",
    "        super(CocoImages, self).__init__()\n",
    "        self.path = path\n",
    "        \n",
    "        id_to_filename = {}\n",
    "        for filename in os.listdir(self.path):\n",
    "            if not filename.endswith('.jpg'):\n",
    "                continue\n",
    "            id_and_extension = filename.split('_')[-1]\n",
    "            id = int(id_and_extension.split('.')[0])\n",
    "            id_to_filename[id] = filename\n",
    "        \n",
    "        self.id_to_filename = id_to_filename\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.sorted_ids = sorted(self.id_to_filename.keys())  # used for deterministic iteration order\n",
    "        print('There are {} images in {}'.format(len(self), self.path))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Scale(int(448 / .875)),\n",
    "            transforms.CenterCrop(448),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        id = self.sorted_ids[item]\n",
    "        path = os.path.join(self.path, self.id_to_filename[id])\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return id, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sorted_ids)\n",
    "    \n",
    "class Composite(data.Dataset):\n",
    "    \"\"\" Dataset that is a composite of several Dataset objects. Useful for combining splits of a dataset. \"\"\"\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        current = self.datasets[0]\n",
    "        for d in self.datasets:\n",
    "            if item < len(d):\n",
    "                return d[item]\n",
    "            item -= len(d)\n",
    "        else:\n",
    "            raise IndexError('Index too large for composite dataset')\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(map(len, self.datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = pretrained_ResNet().cuda()\n",
    "resnet.eval()\n",
    "train_images = CocoImages(train_image_path)\n",
    "val_images = CocoImages(val_image_path)\n",
    "\n",
    "dataset = Composite(train_images, val_images)\n",
    "data_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=0,\n",
    "    shuffle = False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "features_shape = (\n",
    "    len(data_loader.dataset),\n",
    "    output_features,\n",
    "    output_size,\n",
    "    output_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(preprocessed_path, libver='latest') as fd:\n",
    "    features = fd.create_dataset('features', shape=features_shape, dtype='float16')\n",
    "    coco_ids = fd.create_dataset('ids', shape=(len(data_loader.dataset),), dtype='int32')\n",
    "\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for ids, imgs in tqdm(data_loader):\n",
    "            imgs = imgs.cuda()\n",
    "            out = resnet(imgs)\n",
    "            features[i:i + imgs.size(0), :, :] = out.data.cpu().numpy().astype('float16')\n",
    "            coco_ids[i:i + imgs.size(0)] = ids.numpy().astype('int32')\n",
    "            i = i + imgs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocab(iterable, top_k=None, start=0):\n",
    "    \"\"\" Turns an iterable of list of tokens into a vocabulary.\n",
    "        These tokens could be single answers or word tokens in questions.\n",
    "    \"\"\"\n",
    "    all_tokens = itertools.chain.from_iterable(iterable)\n",
    "    counter = Counter(all_tokens)\n",
    "    if top_k:\n",
    "        most_common = counter.most_common(top_k)\n",
    "        most_common = (t for t, c in most_common)\n",
    "    else:\n",
    "        most_common = counter.keys()\n",
    "    # descending in count, then lexicographical order\n",
    "    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n",
    "    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used for normalizing questions\n",
    "_special_chars = re.compile('[^a-z0-9 ]*')\n",
    "\n",
    "# these try to emulate the original normalization scheme for answers\n",
    "_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n",
    "_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n",
    "_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n",
    "_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n",
    "_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n",
    "\n",
    "\n",
    "def prepare_questions(questions_json):\n",
    "    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n",
    "    questions = [q['question'] for q in questions_json['questions']]\n",
    "    for question in questions:\n",
    "        question = question.lower()[:-1]\n",
    "        yield question.split(' ')\n",
    "        \n",
    "def prepare_answers(answers_json):\n",
    "    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n",
    "    answers = [[a['answer'] for a in ans_dict['answers']] for ans_dict in answers_json['annotations']]\n",
    "    # The only normalization that is applied to both machine generated answers as well as\n",
    "    # ground truth answers is replacing most punctuation with space (see [0] and [1]).\n",
    "    # Since potential machine generated answers are just taken from most common answers, applying the other\n",
    "    # normalizations is not needed, assuming that the human answers are already normalized.\n",
    "    # [0]: http://visualqa.org/evaluation.html\n",
    "    # [1]: https://github.com/VT-vision-lab/VQA/blob/3849b1eae04a0ffd83f56ad6f70ebd0767e09e0f/PythonEvaluationTools/vqaEvaluation/vqaEval.py#L96\n",
    "\n",
    "    def process_punctuation(s):\n",
    "        # the original is somewhat broken, so things that look odd here might just be to mimic that behaviour\n",
    "        # this version should be faster since we use re instead of repeated operations on str's\n",
    "        if _punctuation.search(s) is None:\n",
    "            return s\n",
    "        s = _punctuation_with_a_space.sub('', s)\n",
    "        if re.search(_comma_strip, s) is not None:\n",
    "            s = s.replace(',', '')\n",
    "        s = _punctuation.sub(' ', s)\n",
    "        s = _period_strip.sub('', s)\n",
    "        return s.strip()\n",
    "\n",
    "    for answer_list in answers:\n",
    "        yield list(map(process_punctuation, answer_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = prepare_questions(json.load(open(train_questions,'r')))\n",
    "train_answers = prepare_answers(json.load(open(train_answers,'r')))\n",
    "question_vocab = extract_vocab(train_questions, start=1)\n",
    "answer_vocab = extract_vocab(train_answers, top_k=max_answers)\n",
    "vocabs = {\n",
    "    'question': question_vocab,\n",
    "    'answer': answer_vocab,\n",
    "}\n",
    "with open(vocabulary_path, 'w') as fd:\n",
    "    json.dump(vocabs, fd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
